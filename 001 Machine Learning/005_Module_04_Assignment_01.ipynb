{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/zuhayerror3i8/AI-ML-Expert-With-Phitron-Batch-01/blob/main/001%20Machine%20Learning/005_Module_04_Assignment_01.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "88b00676",
      "metadata": {
        "id": "88b00676"
      },
      "source": [
        "\n",
        "# Week 01 Assignment  \n",
        "## Data Quality, Evaluation, Scaling, and Encoding\n",
        "\n",
        "**Student name: MZ AnjumHaQ Heemel**   \n",
        "\n",
        "This is a small assignment that connects topics from Module 1, 2, and 3.  \n",
        "You must complete it in this Colab notebook.\n",
        "\n",
        "You will need to use concepts that appeared in the videos:\n",
        "- Module 1 and 2: basic descriptive statistics, proportions, confusion matrix, accuracy, precision, recall\n",
        "- Module 3: standardization, min max scaling, nominal vs ordinal, one hot encoding, ordinal encoding, Euclidean and Manhattan distance\n",
        "\n",
        "Please do not use any extra libraries beyond `pandas`, `numpy`.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b259ec4e",
      "metadata": {
        "id": "b259ec4e"
      },
      "source": [
        "\n",
        "---\n",
        "## 0. Setup and Dataset\n",
        "\n",
        "We will use a dataset that should have columns given below:\n",
        "\n",
        "- `user_id`  \n",
        "- `age`  \n",
        "- `monthly_income` (numeric)  \n",
        "- `daily_screen_time_min` (numeric)  \n",
        "- `daily_app_opens` (numeric)  \n",
        "- `true_label` and `pred_label` for a binary classification task (0 or 1)  \n",
        "- `satisfaction_level` (for example: `Low`, `Medium`, `High`)  \n",
        "- `city_type` (for example: `Urban`, `Suburban`, `Rural`)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "59e47ae6",
      "metadata": {
        "id": "59e47ae6"
      },
      "outputs": [],
      "source": [
        "# Cell 1: Imports\n",
        "import pandas as pd\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4304549f",
      "metadata": {
        "id": "4304549f"
      },
      "outputs": [],
      "source": [
        "# Cell 2: Load the dataset (Already done for you)\n",
        "df = pd.read_csv(\"https://drive.google.com/uc?export=download&id=1OmDDCh4MD1TtvAemnwVDyz5zwCIXJ220\")\n",
        "\n",
        "# Show first few rows\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "33e827a5",
      "metadata": {
        "id": "33e827a5"
      },
      "source": [
        "\n",
        "### 0.1 Check your dataset\n",
        "\n",
        "1. Confirm that the dataset loaded correctly.  \n",
        "2. Check that you have at least these columns:  \n",
        "   - numeric: `age`, `monthly_income`, `daily_screen_time_min`, `daily_app_opens`  \n",
        "   - labels: `true_label`, `pred_label`  \n",
        "   - categorical: `satisfaction_level`, `city_type`  \n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "740c5e00",
      "metadata": {
        "id": "740c5e00"
      },
      "source": [
        "\n",
        "---\n",
        "## Part A - Module 1 and 2 Review\n",
        "\n",
        "In this part you will do simple descriptive statistics and basic classification evaluation.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2315a46d",
      "metadata": {
        "id": "2315a46d"
      },
      "source": [
        "\n",
        "### Q1. Descriptive statistics on a numeric feature\n",
        "\n",
        "Choose one numeric column, for example `daily_screen_time_min`.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fa6cb0d2",
      "metadata": {
        "id": "fa6cb0d2"
      },
      "outputs": [],
      "source": [
        "# Q1.1: Choose your numeric column here [We already write this ans]\n",
        "num_col = \"daily_screen_time_min\"\n",
        "\n",
        "df[num_col].describe()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8e71d94c",
      "metadata": {
        "id": "8e71d94c"
      },
      "source": [
        "\n",
        "> **Q1.2 Short answer: [Marks: 05]**  \n",
        "> Look at the count, mean, min, max, and standard deviation for your chosen column.  \n",
        "> In 2 to 3 sentences, comment on what you see.  \n",
        "> For example, does the max look very far from the mean, or does it look quite close?\n",
        "\n",
        "Write your answer here:\n",
        "\n",
        ">  The dataset contains 100 observations with complete data for daily screen time.\n",
        "\n",
        ">  The distribution exhibits significant variability, with the minimum value approximately 121 units below the mean and the maximum approximately 117 units above the mean.\n",
        "\n",
        ">  The interquartile range analysis reveals substantial spread, where the 75th percentile is nearly twice the value of the 25th percentile, indicating right-skewed distribution.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "16ab91b6",
      "metadata": {
        "id": "16ab91b6"
      },
      "source": [
        "\n",
        "### Q2. Proportion of positive class\n",
        "\n",
        "Use the `true_label` column, where 1 means \"positive\" and 0 means \"negative\".\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "adcece78",
      "metadata": {
        "id": "adcece78"
      },
      "outputs": [],
      "source": [
        "# Q2.1: Compute proportion of positive class [We already write this ans]\n",
        "label_col = \"true_label\"\n",
        "\n",
        "positive_count = (df[label_col] == 1).sum()\n",
        "total_count = df.shape[0]\n",
        "positive_proportion = positive_count / total_count\n",
        "\n",
        "print(\"Positive count:\", positive_count)\n",
        "print(\"Total samples:\", total_count)\n",
        "print(\"Proportion of positive class:\", positive_proportion)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c065b4b4",
      "metadata": {
        "id": "c065b4b4"
      },
      "source": [
        "\n",
        "> **Q2.2 Short answer: [5 marks]**  \n",
        "> In 1 to 2 sentences, explain what this proportion tells you about your dataset.  \n",
        "> For example, is the dataset balanced between 0 and 1, or is one class much more common?\n",
        "\n",
        "Write your answer here:\n",
        "\n",
        ">  The dataset comprises 100 samples, with 52 instances belonging to the positive class.\n",
        "\n",
        ">  This indicates a relatively balanced dataset with a slight majority (52%) of positive cases, suggesting minimal class imbalance that would require special handling.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "06ff684b",
      "metadata": {
        "id": "06ff684b"
      },
      "source": [
        "\n",
        "### Q3. Confusion matrix and basic metrics\n",
        "\n",
        "For this question, use:\n",
        "- `true_label` as the actual label  \n",
        "- `pred_label` as the model prediction\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e075cdf0",
      "metadata": {
        "id": "e075cdf0"
      },
      "outputs": [],
      "source": [
        "# Q3.1: Manually compute TP, TN, FP, FN [We already write this ans]\n",
        "true_col = \"true_label\"\n",
        "pred_col = \"pred_label\"\n",
        "\n",
        "tp = ((df[true_col] == 1) & (df[pred_col] == 1)).sum()\n",
        "tn = ((df[true_col] == 0) & (df[pred_col] == 0)).sum()\n",
        "fp = ((df[true_col] == 0) & (df[pred_col] == 1)).sum()\n",
        "fn = ((df[true_col] == 1) & (df[pred_col] == 0)).sum()\n",
        "\n",
        "print(\"TP:\", tp)\n",
        "print(\"TN:\", tn)\n",
        "print(\"FP:\", fp)\n",
        "print(\"FN:\", fn)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8857988e",
      "metadata": {
        "id": "8857988e"
      },
      "outputs": [],
      "source": [
        "# Q3.2: Compute accuracy, precision, recall [We already write this ans]\n",
        "accuracy = (tp + tn) / (tp + tn + fp + fn) if (tp + tn + fp + fn) > 0 else 0\n",
        "\n",
        "precision = tp / (tp + fp) if (tp + fp) > 0 else 0\n",
        "recall = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
        "\n",
        "print(\"Accuracy:\", accuracy)\n",
        "print(\"Precision:\", precision)\n",
        "print(\"Recall:\", recall)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0937d45c",
      "metadata": {
        "id": "0937d45c"
      },
      "source": [
        "\n",
        "> **Q3.3 Short answer: [10 marks]**  \n",
        "> In 3 to 4 sentences, briefly comment on the model using these three metrics.  \n",
        "> For example, is the model catching most positives (high recall) or being careful when it predicts positive (high precision)?\n",
        "\n",
        "Write your answer here:\n",
        "\n",
        ">  The model demonstrates moderate performance with an accuracy of 55%, precision of 57%, and recall of 53%.\n",
        "\n",
        ">  The accuracy of 55% suggests the model performs marginally better than random classification, indicating room for substantial improvement.\n",
        "\n",
        ">  Both precision (57%) and recall (53%) are relatively balanced but suboptimal, demonstrating that the model neither excels at identifying true positives nor at avoiding false positives.\n",
        "\n",
        ">  This balanced mediocrity suggests the model requires refinement, as it achieves neither high precision nor high recall characteristics.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f56c651d",
      "metadata": {
        "id": "f56c651d"
      },
      "source": [
        "\n",
        "---\n",
        "## Part B - Module 3: Scaling and Encoding\n",
        "\n",
        "Now we will pick a few features and apply scaling and encoding.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8b0e548b",
      "metadata": {
        "id": "8b0e548b"
      },
      "source": [
        "\n",
        "### Q4. Standardization and Min max scaling\n",
        "\n",
        "Use one numeric column, `monthly_income`.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e8f0c82f",
      "metadata": {
        "id": "e8f0c82f"
      },
      "outputs": [],
      "source": [
        "# Q4.1: Choose the numeric column [2 marks]\n",
        "num_col = df[\"monthly_income\"]\n",
        "\n",
        "num_col"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fba23640",
      "metadata": {
        "id": "fba23640"
      },
      "outputs": [],
      "source": [
        "# Q4.2: Standardization with z-score [10 marks]\n",
        "mean = num_col.mean()\n",
        "std = num_col.std()\n",
        "df[\"z-score\"] = (num_col - mean) / std\n",
        "\n",
        "df[[\"monthly_income\", \"z-score\"]]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dGAqWYZ5l1yU",
      "metadata": {
        "id": "dGAqWYZ5l1yU"
      },
      "outputs": [],
      "source": [
        "df[\"z-score\"].min()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "_X1ZFklnmAHN",
      "metadata": {
        "id": "_X1ZFklnmAHN"
      },
      "outputs": [],
      "source": [
        "df[\"z-score\"].max()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "12ec9ab7",
      "metadata": {
        "id": "12ec9ab7"
      },
      "outputs": [],
      "source": [
        "# Q4.3: Min max scaling implementation [10 marks]\n",
        "mn = num_col.min()\n",
        "mx = num_col.max()\n",
        "rg = mx - mn\n",
        "ss = num_col - mn\n",
        "mm = ss / rg\n",
        "mm = mm.round(2)\n",
        "df[\"min-max-scaling\"] = mm\n",
        "\n",
        "df[[\"monthly_income\", \"min-max-scaling\"]]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "LOuzzsDwmD3p",
      "metadata": {
        "id": "LOuzzsDwmD3p"
      },
      "outputs": [],
      "source": [
        "df[\"min-max-scaling\"].min()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dHwG9XOmmFpz",
      "metadata": {
        "id": "dHwG9XOmmFpz"
      },
      "outputs": [],
      "source": [
        "df[\"min-max-scaling\"].max()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a21a7545",
      "metadata": {
        "id": "a21a7545"
      },
      "source": [
        "\n",
        "> **Q4.4 Short answer: [3 marks]**  \n",
        "> Compare the standardized and min max scaled columns in 2 to 3 sentences.  \n",
        "> Mention what kind of range each one uses and how the numbers look.\n",
        "\n",
        "Write your answer here:\n",
        "\n",
        ">  Standardized columns contain both negative and positive values, while min-max scaled columns are bounded between 0 and 1.\n",
        "\n",
        ">  The standardized values range from -2.09 to 2.40, typically falling within approximately three standard deviations from the mean, whereas min-max scaling produces values strictly between 0 and 1.\n",
        "\n",
        ">  Standardization preserves the distribution shape and handles outliers better, while min-max scaling compresses all values into a fixed range regardless of the original distribution.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "057ed4dd",
      "metadata": {
        "id": "057ed4dd"
      },
      "source": [
        "\n",
        "### Q5. One hot and ordinal encoding\n",
        "\n",
        "We will use:\n",
        "- `city_type` as a nominal feature  \n",
        "- `satisfaction_level` as an ordinal feature with order `Low` < `Medium` < `High`  \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f3f9a90c",
      "metadata": {
        "id": "f3f9a90c"
      },
      "outputs": [],
      "source": [
        "# Q5.1: One hot encoding for city_type using pandas [10 marks]\n",
        "d_city = pd.get_dummies(df[\"city_type\"], prefix=\"city\" ,dtype=int)\n",
        "\n",
        "d_city"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "KZtQxA8VwFdV",
      "metadata": {
        "id": "KZtQxA8VwFdV"
      },
      "outputs": [],
      "source": [
        "# Q5.2: Attach one hot encoded columns to df [5 marks]\n",
        "df = pd.concat([df, d_city], axis=1)\n",
        "\n",
        "df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "71e83a16",
      "metadata": {
        "id": "71e83a16"
      },
      "outputs": [],
      "source": [
        "# Q5.3: Ordinal encoding for satisfaction_level [10 marks]\n",
        "order = {\"Low\":1, \"Medium\":2, \"High\":3}\n",
        "df[\"satisfaction_level\"] = df[\"satisfaction_level\"].map(order).astype(int)\n",
        "\n",
        "df"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a969ca2b",
      "metadata": {
        "id": "a969ca2b"
      },
      "source": [
        "\n",
        "> **Q5.4 Short answer: [5 marks]**  \n",
        "> In 2 to 3 sentences, explain why one hot encoding is suitable for `city_type`  \n",
        "> and why ordinal encoding is suitable for `satisfaction_level`.\n",
        "\n",
        "Write your answer here:\n",
        "\n",
        ">  One-hot encoding is appropriate for `city_type` because city categories (Urban, Suburban, Rural) are nominal variables with no inherent ordering or hierarchical relationship among them.\n",
        "\n",
        ">  Ordinal encoding is suitable for `satisfaction_level` because it represents an ordered categorical variable with a clear ranking system where Low < Medium < High, and the numerical encoding preserves this meaningful ordinality.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d9d76c20",
      "metadata": {
        "id": "d9d76c20"
      },
      "source": [
        "\n",
        "---\n",
        "## Part C - Module 3: Distances between users\n",
        "\n",
        "For this small part we will work with vectors based on scaled numeric features.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ed773931",
      "metadata": {
        "id": "ed773931"
      },
      "source": [
        "\n",
        "### Q6. Euclidean and Manhattan distance\n",
        "\n",
        "Build 2D vectors for user 0 and user 1 using:\n",
        "- `income_std`  \n",
        "- `daily_app_opens` (or its min max scaled version if you prefer)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e52ab645",
      "metadata": {
        "id": "e52ab645"
      },
      "outputs": [],
      "source": [
        "# Q6.1: Build 2D vectors for first two users [We already write this ans]\n",
        "vec_cols = [\"monthly_income\", \"daily_app_opens\"]\n",
        "\n",
        "v1 = df.loc[0, vec_cols].values\n",
        "v2 = df.loc[1, vec_cols].values\n",
        "\n",
        "print(\"v1:\", v1)\n",
        "print(\"v2:\", v2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d887c5b8",
      "metadata": {
        "id": "d887c5b8"
      },
      "outputs": [],
      "source": [
        "# Q6.2: Euclidean distance computation [10 marks]\n",
        "eu = np.linalg.norm(v1 - v2)\n",
        "\n",
        "print(eu)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c7600334",
      "metadata": {
        "id": "c7600334"
      },
      "outputs": [],
      "source": [
        "# Q6.3: Manhattan distance computation [10 marks]\n",
        "ma = np.linalg.norm(v1 - v2, ord=1)\n",
        "\n",
        "print(ma)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c1cdf3e1",
      "metadata": {
        "id": "c1cdf3e1"
      },
      "source": [
        "\n",
        "> **Q6.4 Short answer: [5 marks]**  \n",
        "> Which one is larger in your result, Euclidean or Manhattan distance  \n",
        "> and why does that usually happen based on their formulas?\n",
        "\n",
        "Write your answer here:\n",
        "\n",
        ">  In this analysis, the Manhattan distance is larger than the Euclidean distance.\n",
        "\n",
        ">  This relationship occurs because Manhattan distance computes the sum of absolute differences, while Euclidean distance calculates the square root of the sum of squared differences.\n",
        "\n",
        ">  The square root operation in the Euclidean formula typically produces smaller values than the direct summation used in Manhattan distance, especially when differences are distributed across multiple dimensions.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cb70404e",
      "metadata": {
        "id": "cb70404e"
      },
      "source": [
        "\n",
        "---\n",
        "## Final Reflection [5 marks]\n",
        "\n",
        "> In 4 to 6 sentences, describe how the three modules connect in this assignment.  \n",
        "> Mention:\n",
        "> - One idea from Module 1 or 2 that you used  \n",
        "> - One idea from Module 3 that you used  \n",
        "> - How these ideas together help you understand a dataset more deeply\n",
        "\n",
        "Write your reflection here:\n",
        "\n",
        ">  From Modules 1 and 2, I applied model evaluation metrics including accuracy, precision, and recall to assess classification performance systematically.\n",
        "\n",
        ">  Module 3 concepts included standardization, min-max scaling, categorical encoding techniques, and distance calculations (Euclidean and Manhattan).\n",
        "\n",
        ">  The evaluation metrics from earlier modules provided quantitative insights into model performance and dataset balance, enabling informed assessment of predictive quality.\n",
        "\n",
        ">  Descriptive statistics revealed underlying data patterns and distributions, informing preprocessing decisions.\n",
        "\n",
        ">  Scaling and encoding techniques from Module 3 transformed raw data into formats suitable for machine learning algorithms, bridging the gap between data exploration and model development.\n",
        "\n",
        ">  Distance metrics demonstrated practical applications of transformed features, measuring similarity between observations and connecting preprocessing choices to their analytical consequences.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "48e355bf",
      "metadata": {
        "id": "48e355bf"
      },
      "source": [
        "\n",
        "## End of Assignment\n",
        "\n",
        "Before submitting:\n",
        "- Run all cells from top to bottom.  \n",
        "- Check that all answer sections are filled.  \n",
        "- Download this notebook as `.ipynb` and upload it according to the given instructions.\n",
        "- ***Must Read Assignment Module Text Instruction fully Where you will find how to submit this assignment***\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}