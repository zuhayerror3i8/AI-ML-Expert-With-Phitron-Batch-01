{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/zuhayerror3i8/AI-ML-Expert-With-Phitron-Batch-01/blob/main/001%20Machine%20Learning/004_Module_03_05_Practice_Day_02.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fbb349a9",
      "metadata": {
        "id": "fbb349a9"
      },
      "source": [
        "# Module 3.5 ‚Äî Practice Day 2\n",
        "\n",
        "**Topics Covered:**\n",
        "- Standardization, Min-Max scaling, Robust scaling\n",
        "- Nominal vs ordinal variables, one-hot vs ordinal encoding\n",
        "- Vectors, dot product, norms, Euclidean and Manhattan distance"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3a52b58b",
      "metadata": {
        "id": "3a52b58b"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.preprocessing import StandardScaler, MinMaxScaler, RobustScaler, OneHotEncoder, OrdinalEncoder\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from scipy.spatial import distance"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5f831c09",
      "metadata": {
        "id": "5f831c09"
      },
      "source": [
        "# Part A. Quick Basics\n",
        "\n",
        "## A1. Spot the Right Scaler\n",
        "\n",
        "For each feature, pick one scaler and justify in one line."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "34b272f1",
      "metadata": {
        "id": "34b272f1"
      },
      "outputs": [],
      "source": [
        "print(\"A1. Spot the Right Scaler\\n\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "print(\"\\na) Apartment_price_BDT with a few luxury penthouses\")\n",
        "print(\"   Scaler: Robust Scaler\")\n",
        "print(\"   Justification: Luxury penthouses are outliers; RobustScaler uses median and IQR,\")\n",
        "print(\"                  making it resistant to extreme values.\")\n",
        "\n",
        "print(\"\\nb) Skin_temperature_C measured from a wearable between 30 and 36\")\n",
        "print(\"   Scaler: Min-Max Scaler\")\n",
        "print(\"   Justification: The range is bounded and narrow with no outliers expected;\")\n",
        "print(\"                  Min-Max scales to [0,1] preserving the distribution.\")\n",
        "\n",
        "print(\"\\nc) Daily_app_opens with many zeros and a few power users\")\n",
        "print(\"   Scaler: Robust Scaler or Log Transform + StandardScaler\")\n",
        "print(\"   Justification: Power users are outliers; RobustScaler handles them better,\")\n",
        "print(\"                  or use log transform to reduce skewness before standardizing.\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b1b4c9c0",
      "metadata": {
        "id": "b1b4c9c0"
      },
      "source": [
        "## A2. Manual Min-Max on a Tiny Set\n",
        "\n",
        "Given scores = [20, 25, 30, 50], scale to [0, 1] by hand. Show each step."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7dad65e9",
      "metadata": {
        "id": "7dad65e9"
      },
      "outputs": [],
      "source": [
        "scores = np.array([20, 25, 30, 50])\n",
        "\n",
        "print(\"A2. Manual Min-Max Scaling\\n\")\n",
        "print(f\"Original scores: {scores}\")\n",
        "\n",
        "# Step 1: Find min and max\n",
        "min_val = scores.min()\n",
        "max_val = scores.max()\n",
        "print(f\"\\nStep 1: Find min and max\")\n",
        "print(f\"  min = {min_val}\")\n",
        "print(f\"  max = {max_val}\")\n",
        "\n",
        "# Step 2: Apply formula for each value\n",
        "print(f\"\\nStep 2: Apply formula: X_scaled = (X - min) / (max - min)\")\n",
        "print()\n",
        "\n",
        "scaled_scores = []\n",
        "for score in scores:\n",
        "    scaled = (score - min_val) / (max_val - min_val)\n",
        "    scaled_scores.append(scaled)\n",
        "    print(f\"  {score}: ({score} - {min_val}) / ({max_val} - {min_val}) = {score - min_val} / {max_val - min_val} = {scaled:.4f}\")\n",
        "\n",
        "scaled_scores = np.array(scaled_scores)\n",
        "print(f\"\\nScaled scores: {scaled_scores}\")\n",
        "\n",
        "# Verification with sklearn\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "scaler = MinMaxScaler()\n",
        "sklearn_scaled = scaler.fit_transform(scores.reshape(-1, 1)).flatten()\n",
        "print(f\"Verification (sklearn): {sklearn_scaled}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "06198234",
      "metadata": {
        "id": "06198234"
      },
      "source": [
        "## A3. Z-scores on a Subset\n",
        "\n",
        "Given x = [8, 9, 11], compute mean, standard deviation, then standardize each. Use population standard deviation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "74a31f1d",
      "metadata": {
        "id": "74a31f1d"
      },
      "outputs": [],
      "source": [
        "x = np.array([8, 9, 11])\n",
        "\n",
        "print(\"A3. Z-scores (Standardization)\\n\")\n",
        "print(f\"Original data: {x}\")\n",
        "\n",
        "# Step 1: Compute mean\n",
        "mean = x.mean()\n",
        "print(f\"\\nStep 1: Compute mean\")\n",
        "print(f\"  Œº = ({x[0]} + {x[1]} + {x[2]}) / 3 = {x.sum()} / 3 = {mean:.4f}\")\n",
        "\n",
        "# Step 2: Compute population standard deviation (ddof=0)\n",
        "std = x.std(ddof=0)\n",
        "variance = x.var(ddof=0)\n",
        "print(f\"\\nStep 2: Compute population standard deviation\")\n",
        "print(f\"  Variance œÉ¬≤ = [(8-{mean:.4f})¬≤ + (9-{mean:.4f})¬≤ + (11-{mean:.4f})¬≤] / 3\")\n",
        "print(f\"  Variance œÉ¬≤ = [{(8-mean)**2:.4f} + {(9-mean)**2:.4f} + {(11-mean)**2:.4f}] / 3\")\n",
        "print(f\"  Variance œÉ¬≤ = {variance:.4f}\")\n",
        "print(f\"  œÉ = ‚àö{variance:.4f} = {std:.4f}\")\n",
        "\n",
        "# Step 3: Standardize each value\n",
        "print(f\"\\nStep 3: Standardize each value using Z = (X - Œº) / œÉ\")\n",
        "z_scores = []\n",
        "for val in x:\n",
        "    z = (val - mean) / std\n",
        "    z_scores.append(z)\n",
        "    print(f\"  {val}: ({val} - {mean:.4f}) / {std:.4f} = {z:.4f}\")\n",
        "\n",
        "z_scores = np.array(z_scores)\n",
        "print(f\"\\nZ-scores: {z_scores}\")\n",
        "print(f\"Verification - Mean of Z-scores: {z_scores.mean():.6f} (should be ~0)\")\n",
        "print(f\"Verification - Std of Z-scores: {z_scores.std(ddof=0):.6f} (should be ~1)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "617126fd",
      "metadata": {
        "id": "617126fd"
      },
      "source": [
        "## A4. Robust Scaling Ingredients\n",
        "\n",
        "Given y = [5, 6, 6, 7, 50], find median, Q1, Q3, IQR. Do not scale yet."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "42c5cd8b",
      "metadata": {
        "id": "42c5cd8b"
      },
      "outputs": [],
      "source": [
        "y = np.array([5, 6, 6, 7, 50])\n",
        "\n",
        "print(\"A4. Robust Scaling Ingredients\\n\")\n",
        "print(f\"Data: {y}\")\n",
        "print(f\"Sorted data: {np.sort(y)}\")\n",
        "\n",
        "# Calculate quartiles\n",
        "Q1 = np.percentile(y, 25)\n",
        "Q2 = np.percentile(y, 50)  # median\n",
        "Q3 = np.percentile(y, 75)\n",
        "IQR = Q3 - Q1\n",
        "\n",
        "print(f\"\\nMedian (Q2): {Q2}\")\n",
        "print(f\"Q1 (25th percentile): {Q1}\")\n",
        "print(f\"Q3 (75th percentile): {Q3}\")\n",
        "print(f\"IQR (Q3 - Q1): {Q3} - {Q1} = {IQR}\")\n",
        "\n",
        "print(f\"\\nNote: The value 50 is an outlier (much larger than Q3 = {Q3}).\")\n",
        "print(f\"Robust scaling formula: X_scaled = (X - median) / IQR\")\n",
        "print(f\"This makes the scaled values resistant to the outlier at 50.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1cb50eed",
      "metadata": {
        "id": "1cb50eed"
      },
      "source": [
        "## A5. Nominal or Ordinal\n",
        "\n",
        "Mark each as nominal or ordinal."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "726e060e",
      "metadata": {
        "id": "726e060e"
      },
      "outputs": [],
      "source": [
        "print(\"A5. Nominal or Ordinal?\\n\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "print(\"\\na) T-shirt_size {S, M, L, XL}\")\n",
        "print(\"   Type: ORDINAL\")\n",
        "print(\"   Reason: There is a clear ordering: S < M < L < XL (small to extra large)\")\n",
        "\n",
        "print(\"\\nb) City {Dhaka, Chattogram, Rajshahi}\")\n",
        "print(\"   Type: NOMINAL\")\n",
        "print(\"   Reason: No inherent ordering or ranking between cities\")\n",
        "\n",
        "print(\"\\nc) Satisfaction {Low, Medium, High}\")\n",
        "print(\"   Type: ORDINAL\")\n",
        "print(\"   Reason: Clear ordering: Low < Medium < High\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"\\nKey Difference:\")\n",
        "print(\"  ‚Ä¢ Nominal: Categories with NO natural order (use One-Hot Encoding)\")\n",
        "print(\"  ‚Ä¢ Ordinal: Categories with MEANINGFUL order (use Ordinal Encoding)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0b150c28",
      "metadata": {
        "id": "0b150c28"
      },
      "source": [
        "# Part B. Hands-On Practice\n",
        "\n",
        "## B1. Three Scalers Side by Side\n",
        "\n",
        "Heights = [150, 160, 170, 175, 180]  \n",
        "Weights = [58, 62, 65, 66, 190]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "933e2b36",
      "metadata": {
        "id": "933e2b36"
      },
      "outputs": [],
      "source": [
        "heights = np.array([150, 160, 170, 175, 180])\n",
        "weights = np.array([58, 62, 65, 66, 190])\n",
        "\n",
        "print(\"B1. Three Scalers Side by Side\\n\")\n",
        "print(f\"Heights: {heights}\")\n",
        "print(f\"Weights: {weights} (Note: 190 is an outlier)\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "# Task a) Min-Max scale both to [0, 1]\n",
        "print(\"\\na) Min-Max Scaling to [0, 1]\")\n",
        "heights_minmax = (heights - heights.min()) / (heights.max() - heights.min())\n",
        "weights_minmax = (weights - weights.min()) / (weights.max() - weights.min())\n",
        "print(f\"   Heights (Min-Max): {heights_minmax}\")\n",
        "print(f\"   Weights (Min-Max): {weights_minmax}\")\n",
        "print(f\"   Note: Outlier 190 dominates weights; 66‚Üí0.0606 instead of being near 1.0\")\n",
        "\n",
        "# Task b) Standardize the first three values of each\n",
        "print(\"\\nb) Standardize First Three Values Only\")\n",
        "heights_first3 = heights[:3]\n",
        "weights_first3 = weights[:3]\n",
        "heights_std = (heights_first3 - heights_first3.mean()) / heights_first3.std(ddof=1)\n",
        "weights_std = (weights_first3 - weights_first3.mean()) / weights_first3.std(ddof=1)\n",
        "print(f\"   Heights[0:3]: {heights_first3}\")\n",
        "print(f\"   Standardized: {heights_std}\")\n",
        "print(f\"   Weights[0:3]: {weights_first3}\")\n",
        "print(f\"   Standardized: {weights_std}\")\n",
        "\n",
        "# Task c) Robust scale Weights with median and IQR\n",
        "print(\"\\nc) Robust Scale Weights\")\n",
        "median_w = np.median(weights)\n",
        "Q1_w = np.percentile(weights, 25)\n",
        "Q3_w = np.percentile(weights, 75)\n",
        "IQR_w = Q3_w - Q1_w\n",
        "weights_robust = (weights - median_w) / IQR_w\n",
        "print(f\"   Median: {median_w}, Q1: {Q1_w}, Q3: {Q3_w}, IQR: {IQR_w}\")\n",
        "print(f\"   Weights (Robust): {weights_robust}\")\n",
        "print(f\"   Note: Outlier 190 ‚Üí {weights_robust[-1]:.2f}, less extreme than Min-Max\")\n",
        "\n",
        "# Task d) Which scaler handles outlier best\n",
        "print(\"\\nd) Outlier Handling Comparison\")\n",
        "print(f\"   Min-Max:  Outlier pulls all other values to near 0 (bad)\")\n",
        "print(f\"   Standard: Outlier inflates std, distorting all Z-scores\")\n",
        "print(f\"   Robust:   Outlier has minimal effect; uses median/IQR (BEST)\")\n",
        "print(f\"   Winner: Robust Scaler handles the outlier best!\")\n",
        "\n",
        "# Visualization\n",
        "fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
        "\n",
        "axes[0].scatter(heights_minmax, weights_minmax, s=100)\n",
        "axes[0].set_title('Min-Max Scaled')\n",
        "axes[0].set_xlabel('Heights')\n",
        "axes[0].set_ylabel('Weights')\n",
        "axes[0].grid(True, alpha=0.3)\n",
        "\n",
        "axes[1].scatter(heights, (weights - weights.mean()) / weights.std(), s=100, color='orange')\n",
        "axes[1].set_title('Standardized (Z-score)')\n",
        "axes[1].set_xlabel('Heights (original)')\n",
        "axes[1].set_ylabel('Weights (standardized)')\n",
        "axes[1].grid(True, alpha=0.3)\n",
        "\n",
        "axes[2].scatter(heights, weights_robust, s=100, color='green')\n",
        "axes[2].set_title('Robust Scaled')\n",
        "axes[2].set_xlabel('Heights (original)')\n",
        "axes[2].set_ylabel('Weights (robust)')\n",
        "axes[2].grid(True, alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "63efefdb",
      "metadata": {
        "id": "63efefdb"
      },
      "source": [
        "## B2. One-Hot Encoding by Hand\n",
        "\n",
        "Cities = [Dhaka, Chattogram, Dhaka, Rajshahi, Rajshahi]  \n",
        "Create three columns: City_Dhaka, City_Chattogram, City_Rajshahi using 0 and 1."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d86834eb",
      "metadata": {
        "id": "d86834eb"
      },
      "outputs": [],
      "source": [
        "cities = ['Dhaka', 'Chattogram', 'Dhaka', 'Rajshahi', 'Rajshahi']\n",
        "\n",
        "print(\"B2. One-Hot Encoding by Hand\\n\")\n",
        "print(f\"Original: {cities}\")\n",
        "print(\"\\nManual One-Hot Encoding:\")\n",
        "\n",
        "# Create DataFrame manually\n",
        "one_hot_data = {\n",
        "    'City_Dhaka': [],\n",
        "    'City_Chattogram': [],\n",
        "    'City_Rajshahi': []\n",
        "}\n",
        "\n",
        "for city in cities:\n",
        "    one_hot_data['City_Dhaka'].append(1 if city == 'Dhaka' else 0)\n",
        "    one_hot_data['City_Chattogram'].append(1 if city == 'Chattogram' else 0)\n",
        "    one_hot_data['City_Rajshahi'].append(1 if city == 'Rajshahi' else 0)\n",
        "\n",
        "df_onehot = pd.DataFrame(one_hot_data)\n",
        "df_onehot.index.name = 'Row'\n",
        "\n",
        "print(df_onehot)\n",
        "\n",
        "print(\"\\nExplanation:\")\n",
        "print(\"  ‚Ä¢ Each city becomes a separate binary column\")\n",
        "print(\"  ‚Ä¢ Only one column has 1 per row (the city for that row)\")\n",
        "print(\"  ‚Ä¢ This avoids imposing false ordering on nominal categories\")\n",
        "\n",
        "# Verification with sklearn\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "encoder = OneHotEncoder(sparse_output=False)\n",
        "cities_array = np.array(cities).reshape(-1, 1)\n",
        "encoded = encoder.fit_transform(cities_array)\n",
        "print(\"\\nVerification with sklearn:\")\n",
        "print(pd.DataFrame(encoded, columns=encoder.get_feature_names_out(['City'])))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7159da96",
      "metadata": {
        "id": "7159da96"
      },
      "source": [
        "## B3. Ordinal Mapping\n",
        "\n",
        "Education = [High School, Bachelor, Master, Bachelor, Master]  \n",
        "Map with High School=0, Bachelor=1, Master=2.  \n",
        "Then change the map to High School=1, Bachelor=2, Master=3."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "aa116351",
      "metadata": {
        "id": "aa116351"
      },
      "outputs": [],
      "source": [
        "education = ['High School', 'Bachelor', 'Master', 'Bachelor', 'Master']\n",
        "\n",
        "print(\"B3. Ordinal Mapping\\n\")\n",
        "print(f\"Original: {education}\\n\")\n",
        "\n",
        "# First mapping: 0, 1, 2\n",
        "map1 = {'High School': 0, 'Bachelor': 1, 'Master': 2}\n",
        "encoded1 = [map1[e] for e in education]\n",
        "print(\"Mapping 1: High School=0, Bachelor=1, Master=2\")\n",
        "print(f\"Encoded: {encoded1}\")\n",
        "\n",
        "# Second mapping: 1, 2, 3\n",
        "map2 = {'High School': 1, 'Bachelor': 2, 'Master': 3}\n",
        "encoded2 = [map2[e] for e in education]\n",
        "print(\"\\nMapping 2: High School=1, Bachelor=2, Master=3\")\n",
        "print(f\"Encoded: {encoded2}\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"Effect on Distances:\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "# Calculate distances between first two samples\n",
        "print(\"\\nDistance between Sample 0 (High School) and Sample 1 (Bachelor):\")\n",
        "print(f\"  Mapping 1: |{encoded1[0]} - {encoded1[1]}| = {abs(encoded1[0] - encoded1[1])}\")\n",
        "print(f\"  Mapping 2: |{encoded2[0]} - {encoded2[1]}| = {abs(encoded2[0] - encoded2[1])}\")\n",
        "print(f\"  ‚Üí Same difference (1 unit)\")\n",
        "\n",
        "print(\"\\nDistance between Sample 0 (High School) and Sample 2 (Master):\")\n",
        "print(f\"  Mapping 1: |{encoded1[0]} - {encoded1[2]}| = {abs(encoded1[0] - encoded1[2])}\")\n",
        "print(f\"  Mapping 2: |{encoded2[0]} - {encoded2[2]}| = {abs(encoded2[0] - encoded2[2])}\")\n",
        "print(f\"  ‚Üí Same difference (2 units)\")\n",
        "\n",
        "print(\"\\nüìù One-line explanation:\")\n",
        "print(\"   Shifting the mapping by a constant (0‚Üí1, 1‚Üí2, 2‚Üí3) preserves relative\")\n",
        "print(\"   distances, so ordinal relationships remain intact; only absolute values shift.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "747067a6",
      "metadata": {
        "id": "747067a6"
      },
      "source": [
        "## B4. Encoding Mixup [Optional]\n",
        "\n",
        "You mistakenly apply ordinal encoding to City and one-hot to Education. Write one sentence on the risk this creates in a linear model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1c10e610",
      "metadata": {
        "id": "1c10e610"
      },
      "outputs": [],
      "source": [
        "print(\"B4. Encoding Mixup Risk\\n\")\n",
        "print(\"=\"*80)\n",
        "print(\"\\nScenario: Ordinal encoding on City (nominal) and One-hot on Education (ordinal)\")\n",
        "print(\"\\nExample:\")\n",
        "\n",
        "# Wrong: Ordinal encoding on City\n",
        "cities_wrong = ['Dhaka', 'Chattogram', 'Rajshahi']\n",
        "city_map = {'Dhaka': 0, 'Chattogram': 1, 'Rajshahi': 2}\n",
        "print(f\"  City (nominal) with ordinal: Dhaka=0, Chattogram=1, Rajshahi=2\")\n",
        "print(f\"  Problem: Implies Dhaka < Chattogram < Rajshahi (FALSE ordering!)\")\n",
        "\n",
        "# Wrong: One-hot on Education\n",
        "print(f\"\\n  Education (ordinal) with one-hot: 3 separate columns\")\n",
        "print(f\"  Problem: Loses the ordering High School < Bachelor < Master\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"üìù ONE SENTENCE RISK:\")\n",
        "print(\"=\"*80)\n",
        "print(\"\\n  Ordinal encoding City imposes a false ranking (e.g., Dhaka=0 < Rajshahi=2),\")\n",
        "print(\"  while one-hot encoding Education destroys the meaningful order, causing the\")\n",
        "print(\"  linear model to misinterpret relationships and produce unreliable predictions.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "73963ac2",
      "metadata": {
        "id": "73963ac2"
      },
      "source": [
        "## B5. Vectors and Alignment [Optional]\n",
        "\n",
        "a = [3, ‚àí1, 2], b = [4, 0, ‚àí2], c = [‚àí6, 2, ‚àí4]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3915cc1d",
      "metadata": {
        "id": "3915cc1d"
      },
      "outputs": [],
      "source": [
        "a = np.array([3, -1, 2])\n",
        "b = np.array([4, 0, -2])\n",
        "c = np.array([-6, 2, -4])\n",
        "\n",
        "print(\"B5. Vectors and Alignment\\n\")\n",
        "print(f\"a = {a}\")\n",
        "print(f\"b = {b}\")\n",
        "print(f\"c = {c}\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "# Task a) Compute dot products\n",
        "dot_ab = np.dot(a, b)\n",
        "dot_ac = np.dot(a, c)\n",
        "\n",
        "print(\"\\na) Dot Products\")\n",
        "print(f\"   a¬∑b = (3)(4) + (-1)(0) + (2)(-2) = 12 + 0 - 4 = {dot_ab}\")\n",
        "print(f\"   a¬∑c = (3)(-6) + (-1)(2) + (2)(-4) = -18 - 2 - 8 = {dot_ac}\")\n",
        "\n",
        "# Task b) Compare signs and magnitudes\n",
        "print(\"\\nb) Alignment Analysis\")\n",
        "print(f\"   a¬∑b = {dot_ab} (positive) ‚Üí vectors point in similar direction (acute angle)\")\n",
        "print(f\"   |a¬∑b| = {abs(dot_ab)}\")\n",
        "print(f\"\\n   a¬∑c = {dot_ac} (negative) ‚Üí vectors point in opposite directions (obtuse angle)\")\n",
        "print(f\"   |a¬∑c| = {abs(dot_ac)}\")\n",
        "print(f\"\\n   Magnitude comparison: |a¬∑c| = {abs(dot_ac)} > |a¬∑b| = {abs(dot_ab)}\")\n",
        "print(f\"   ‚Üí 'c' is more strongly anti-aligned with 'a' than 'b' is aligned with 'a'\")\n",
        "\n",
        "# Check if c is a scalar multiple of a\n",
        "print(f\"\\n   Note: c = -2 √ó a? Let's check: -2 √ó a = {-2 * a}\")\n",
        "print(f\"   Yes! c and a are parallel but opposite (c = -2a)\")\n",
        "\n",
        "# Task c) L2 normalize a\n",
        "norm_a = np.linalg.norm(a)  # L2 norm\n",
        "a_normalized = a / norm_a\n",
        "\n",
        "print(f\"\\nc) L2 Normalization of a\")\n",
        "print(f\"   L2 norm of a: ||a|| = ‚àö(3¬≤ + (-1)¬≤ + 2¬≤) = ‚àö(9 + 1 + 4) = ‚àö14 = {norm_a:.6f}\")\n",
        "print(f\"   Normalized vector: a_norm = a / ||a||\")\n",
        "print(f\"   a_norm = [{a[0]}/{norm_a:.3f}, {a[1]}/{norm_a:.3f}, {a[2]}/{norm_a:.3f}]\")\n",
        "print(f\"   a_norm = [{a_normalized[0]:.3f}, {a_normalized[1]:.3f}, {a_normalized[2]:.3f}]\")\n",
        "print(f\"\\n   Verification: ||a_norm|| = {np.linalg.norm(a_normalized):.6f} (should be 1.0)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5075ae08",
      "metadata": {
        "id": "5075ae08"
      },
      "source": [
        "## B6. Two Distances, Different Vibes\n",
        "\n",
        "Points: P1(2, 3), P2(5, 7), P3(2, 10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "164fc5ef",
      "metadata": {
        "id": "164fc5ef"
      },
      "outputs": [],
      "source": [
        "P1 = np.array([2, 3])\n",
        "P2 = np.array([5, 7])\n",
        "P3 = np.array([2, 10])\n",
        "\n",
        "print(\"B6. Two Distances, Different Vibes\\n\")\n",
        "print(f\"P1 = {P1}\")\n",
        "print(f\"P2 = {P2}\")\n",
        "print(f\"P3 = {P3}\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "# Task a) Compute Euclidean and Manhattan distances for all pairs\n",
        "print(\"\\na) Distance Calculations\")\n",
        "\n",
        "# Euclidean: sqrt((x2-x1)¬≤ + (y2-y1)¬≤)\n",
        "euclidean_12 = np.sqrt((P2[0]-P1[0])**2 + (P2[1]-P1[1])**2)\n",
        "euclidean_13 = np.sqrt((P3[0]-P1[0])**2 + (P3[1]-P1[1])**2)\n",
        "euclidean_23 = np.sqrt((P3[0]-P2[0])**2 + (P3[1]-P2[1])**2)\n",
        "\n",
        "# Manhattan: |x2-x1| + |y2-y1|\n",
        "manhattan_12 = abs(P2[0]-P1[0]) + abs(P2[1]-P1[1])\n",
        "manhattan_13 = abs(P3[0]-P1[0]) + abs(P3[1]-P1[1])\n",
        "manhattan_23 = abs(P3[0]-P2[0]) + abs(P3[1]-P2[1])\n",
        "\n",
        "print(\"\\n   P1 to P2:\")\n",
        "print(f\"      Euclidean: ‚àö((5-2)¬≤ + (7-3)¬≤) = ‚àö(9 + 16) = ‚àö25 = {euclidean_12:.3f}\")\n",
        "print(f\"      Manhattan: |5-2| + |7-3| = 3 + 4 = {manhattan_12:.3f}\")\n",
        "\n",
        "print(\"\\n   P1 to P3:\")\n",
        "print(f\"      Euclidean: ‚àö((2-2)¬≤ + (10-3)¬≤) = ‚àö(0 + 49) = ‚àö49 = {euclidean_13:.3f}\")\n",
        "print(f\"      Manhattan: |2-2| + |10-3| = 0 + 7 = {manhattan_13:.3f}\")\n",
        "\n",
        "print(\"\\n   P2 to P3:\")\n",
        "print(f\"      Euclidean: ‚àö((2-5)¬≤ + (10-7)¬≤) = ‚àö(9 + 9) = ‚àö18 = {euclidean_23:.3f}\")\n",
        "print(f\"      Manhattan: |2-5| + |10-7| = 3 + 3 = {manhattan_23:.3f}\")\n",
        "\n",
        "# Task b) Which distance is more sensitive to a single large jump\n",
        "print(\"\\nb) Sensitivity to Single Large Jump\")\n",
        "print(\"   Manhattan distance is LINEAR in each dimension (sum of absolute differences)\")\n",
        "print(\"   Euclidean distance SQUARES differences before summing\")\n",
        "print(\"   ‚Üí Euclidean is MORE SENSITIVE to large jumps in a single coordinate\")\n",
        "print(\"   Example: P1‚ÜíP3 has 7-unit jump in y only:\")\n",
        "print(f\"      Manhattan treats it as: 0 + 7 = {manhattan_13}\")\n",
        "print(f\"      Euclidean treats it as: ‚àö(0¬≤ + 7¬≤) = {euclidean_13}\")\n",
        "print(\"      Both give same result here, but if we had [0, 7] vs [5, 5]:\")\n",
        "print(\"         Manhattan: both = 7\")\n",
        "print(\"         Euclidean: [0,7]=7.0 vs [5,5]=7.07 ‚Üí more balanced spread preferred\")\n",
        "\n",
        "# Task c) Scale y by 10 and recompute d(P1, P2)\n",
        "P1_scaled = np.array([P1[0], P1[1] * 10])\n",
        "P2_scaled = np.array([P2[0], P2[1] * 10])\n",
        "\n",
        "euclidean_12_scaled = np.sqrt((P2_scaled[0]-P1_scaled[0])**2 + (P2_scaled[1]-P1_scaled[1])**2)\n",
        "manhattan_12_scaled = abs(P2_scaled[0]-P1_scaled[0]) + abs(P2_scaled[1]-P1_scaled[1])\n",
        "\n",
        "print(\"\\nc) Effect of Scaling Y by 10\")\n",
        "print(f\"   Original: P1={P1}, P2={P2}\")\n",
        "print(f\"   Scaled:   P1={P1_scaled}, P2={P2_scaled}\")\n",
        "print(f\"\\n   Original distances P1‚ÜíP2:\")\n",
        "print(f\"      Euclidean: {euclidean_12:.3f}\")\n",
        "print(f\"      Manhattan: {manhattan_12:.3f}\")\n",
        "print(f\"\\n   Scaled distances P1‚ÜíP2:\")\n",
        "print(f\"      Euclidean: {euclidean_12_scaled:.3f} (was {euclidean_12:.3f})\")\n",
        "print(f\"      Manhattan: {manhattan_12_scaled:.3f} (was {manhattan_12:.3f})\")\n",
        "print(f\"\\n   üìù One-line explanation:\")\n",
        "print(f\"      Scaling y by 10 amplifies y-differences, making y dominate both distance\")\n",
        "print(f\"      metrics and distorting similarity; features must be scaled to equal ranges.\")\n",
        "\n",
        "# Visualization\n",
        "fig, axes = plt.subplots(1, 2, figsize=(14, 6))\n",
        "\n",
        "# Original scale\n",
        "axes[0].scatter(*P1, s=200, c='red', marker='o', label='P1', edgecolors='black', linewidths=2)\n",
        "axes[0].scatter(*P2, s=200, c='blue', marker='s', label='P2', edgecolors='black', linewidths=2)\n",
        "axes[0].scatter(*P3, s=200, c='green', marker='^', label='P3', edgecolors='black', linewidths=2)\n",
        "axes[0].plot([P1[0], P2[0]], [P1[1], P2[1]], 'k--', alpha=0.5, label=f'Euclidean P1-P2: {euclidean_12:.2f}')\n",
        "axes[0].set_xlabel('X')\n",
        "axes[0].set_ylabel('Y')\n",
        "axes[0].set_title('Original Scale')\n",
        "axes[0].legend()\n",
        "axes[0].grid(True, alpha=0.3)\n",
        "axes[0].axis('equal')\n",
        "\n",
        "# Scaled (y √ó 10)\n",
        "axes[1].scatter(*P1_scaled, s=200, c='red', marker='o', label='P1 scaled', edgecolors='black', linewidths=2)\n",
        "axes[1].scatter(*P2_scaled, s=200, c='blue', marker='s', label='P2 scaled', edgecolors='black', linewidths=2)\n",
        "axes[1].plot([P1_scaled[0], P2_scaled[0]], [P1_scaled[1], P2_scaled[1]], 'k--', alpha=0.5,\n",
        "             label=f'Euclidean P1-P2: {euclidean_12_scaled:.2f}')\n",
        "axes[1].set_xlabel('X')\n",
        "axes[1].set_ylabel('Y (scaled √ó10)')\n",
        "axes[1].set_title('Y Scaled by 10')\n",
        "axes[1].legend()\n",
        "axes[1].grid(True, alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "817a3697",
      "metadata": {
        "id": "817a3697"
      },
      "source": [
        "# Part C. Mini Datasets\n",
        "\n",
        "## C-Data-1 and C-Data-2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6c1dd13a",
      "metadata": {
        "id": "6c1dd13a"
      },
      "outputs": [],
      "source": [
        "# C-Data-1\n",
        "data1 = {\n",
        "    'ID': [1, 2, 3, 4, 5],\n",
        "    'Age': [20, 21, 22, 20, 23],\n",
        "    'Hours_Study': [1.0, 0.5, 2.2, 5.0, 0.2],\n",
        "    'GPA': [3.10, 2.60, 3.40, 3.90, 2.30],\n",
        "    'Internet': ['Yes', 'No', 'Yes', 'Yes', 'No'],\n",
        "    'City': ['Dhaka', 'Chattogram', 'Rajshahi', 'Dhaka', 'Rajshahi']\n",
        "}\n",
        "df1 = pd.DataFrame(data1)\n",
        "\n",
        "# C-Data-2\n",
        "data2 = {\n",
        "    'ID': [1, 2, 3, 4, 5],\n",
        "    'Income_BDT': [30000, 45000, 52000, 300000, 38000],\n",
        "    'Transactions': [0, 1, 2, 12, 0],\n",
        "    'Temp_C': [25.0, 26.0, 24.5, 28.0, 25.5],\n",
        "    'Education': ['High School', 'Bachelor', 'Master', 'Bachelor', 'Master'],\n",
        "    'Satisfaction': ['Low', 'Medium', 'High', 'Medium', 'Medium']\n",
        "}\n",
        "df2 = pd.DataFrame(data2)\n",
        "\n",
        "print(\"C-Data-1:\")\n",
        "print(df1)\n",
        "print(\"\\nC-Data-2:\")\n",
        "print(df2)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e9e0b451",
      "metadata": {
        "id": "e9e0b451"
      },
      "source": [
        "## C1. Scaler Choices with Evidence\n",
        "\n",
        "Pick a scaler for Income_BDT, Transactions, Temp_C. For each, give a one-line justification and a two-line numeric illustration using C-Data-2 values."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b6e500dd",
      "metadata": {
        "id": "b6e500dd"
      },
      "outputs": [],
      "source": [
        "print(\"C1. Scaler Choices with Evidence\\n\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "# Income_BDT\n",
        "income = df2['Income_BDT'].values\n",
        "print(\"\\n1. Income_BDT: [30000, 45000, 52000, 300000, 38000]\")\n",
        "print(\"   Scaler: Robust Scaler\")\n",
        "print(\"   Justification: 300000 is a severe outlier (6.5√ó larger than median)\")\n",
        "print(f\"   Illustration:\")\n",
        "print(f\"      Median: {np.median(income)}, IQR: {np.percentile(income, 75) - np.percentile(income, 25)}\")\n",
        "print(f\"      Robust scaled: {(income - np.median(income)) / (np.percentile(income, 75) - np.percentile(income, 25))}\")\n",
        "\n",
        "# Transactions\n",
        "transactions = df2['Transactions'].values\n",
        "print(\"\\n2. Transactions: [0, 1, 2, 12, 0]\")\n",
        "print(\"   Scaler: Robust Scaler or Log Transform + StandardScaler\")\n",
        "print(\"   Justification: 12 is an outlier (6√ó larger than second highest); many zeros\")\n",
        "print(f\"   Illustration:\")\n",
        "print(f\"      Median: {np.median(transactions)}, IQR: {np.percentile(transactions, 75) - np.percentile(transactions, 25)}\")\n",
        "print(f\"      Robust scaled: {(transactions - np.median(transactions)) / max(1, np.percentile(transactions, 75) - np.percentile(transactions, 25))}\")\n",
        "\n",
        "# Temp_C\n",
        "temp = df2['Temp_C'].values\n",
        "print(\"\\n3. Temp_C: [25.0, 26.0, 24.5, 28.0, 25.5]\")\n",
        "print(\"   Scaler: Min-Max Scaler\")\n",
        "print(\"   Justification: Bounded, narrow range (24.5-28¬∞C) with no outliers\")\n",
        "print(f\"   Illustration:\")\n",
        "print(f\"      Min: {temp.min()}, Max: {temp.max()}, Range: {temp.max() - temp.min()}\")\n",
        "print(f\"      Min-Max scaled: {(temp - temp.min()) / (temp.max() - temp.min())}\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1cb43fef",
      "metadata": {
        "id": "1cb43fef"
      },
      "source": [
        "## C2. Mixed Preprocessing Plan\n",
        "\n",
        "For C-Data-1 and C-Data-2 combined:\n",
        "- Identify nominal and ordinal columns\n",
        "- Propose one encoding plan listing exact columns to one-hot vs ordinal\n",
        "- Propose one scaling plan listing exact columns to Min-Max vs Standardization vs Robust"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "42138582",
      "metadata": {
        "id": "42138582"
      },
      "outputs": [],
      "source": [
        "print(\"C2. Mixed Preprocessing Plan\\n\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "print(\"\\na) Identify Nominal and Ordinal Columns\")\n",
        "print(\"\\n   C-Data-1:\")\n",
        "print(\"      Nominal:  Internet (Yes/No), City (Dhaka/Chattogram/Rajshahi)\")\n",
        "print(\"      Ordinal:  None\")\n",
        "print(\"      Numeric:  Age, Hours_Study, GPA\")\n",
        "\n",
        "print(\"\\n   C-Data-2:\")\n",
        "print(\"      Nominal:  None\")\n",
        "print(\"      Ordinal:  Education (High School < Bachelor < Master)\")\n",
        "print(\"                Satisfaction (Low < Medium < High)\")\n",
        "print(\"      Numeric:  Income_BDT, Transactions, Temp_C\")\n",
        "\n",
        "print(\"\\n\" + \"-\"*80)\n",
        "print(\"\\nb) Encoding Plan\")\n",
        "print(\"\\n   One-Hot Encoding:\")\n",
        "print(\"      ‚Ä¢ Internet (binary: Yes/No)\")\n",
        "print(\"      ‚Ä¢ City (3 categories: Dhaka, Chattogram, Rajshahi)\")\n",
        "\n",
        "print(\"\\n   Ordinal Encoding:\")\n",
        "print(\"      ‚Ä¢ Education: {'High School': 0, 'Bachelor': 1, 'Master': 2}\")\n",
        "print(\"      ‚Ä¢ Satisfaction: {'Low': 0, 'Medium': 1, 'High': 2}\")\n",
        "\n",
        "print(\"\\n\" + \"-\"*80)\n",
        "print(\"\\nc) Scaling Plan\")\n",
        "print(\"\\n   Min-Max Scaling:\")\n",
        "print(\"      ‚Ä¢ Age (narrow range 20-23, no outliers)\")\n",
        "print(\"      ‚Ä¢ GPA (bounded range 2.3-3.9, no outliers)\")\n",
        "print(\"      ‚Ä¢ Temp_C (bounded range 24.5-28, no outliers)\")\n",
        "\n",
        "print(\"\\n   Standardization:\")\n",
        "print(\"      ‚Ä¢ Hours_Study (wide range but continuous, bell-shaped expected)\")\n",
        "\n",
        "print(\"\\n   Robust Scaling:\")\n",
        "print(\"      ‚Ä¢ Income_BDT (outlier at 300000)\")\n",
        "print(\"      ‚Ä¢ Transactions (outlier at 12, many zeros)\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "\n",
        "# Create summary table\n",
        "summary = pd.DataFrame({\n",
        "    'Column': ['Age', 'Hours_Study', 'GPA', 'Income_BDT', 'Transactions',\n",
        "               'Temp_C', 'Internet', 'City', 'Education', 'Satisfaction'],\n",
        "    'Type': ['Numeric', 'Numeric', 'Numeric', 'Numeric', 'Numeric',\n",
        "             'Numeric', 'Nominal', 'Nominal', 'Ordinal', 'Ordinal'],\n",
        "    'Preprocessing': ['Min-Max', 'Standard', 'Min-Max', 'Robust', 'Robust',\n",
        "                     'Min-Max', 'One-Hot', 'One-Hot', 'Ordinal', 'Ordinal']\n",
        "})\n",
        "\n",
        "print(\"\\nSummary Table:\")\n",
        "print(summary.to_string(index=False))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b06fec53",
      "metadata": {
        "id": "b06fec53"
      },
      "source": [
        "## C3. Outlier Stress Test [Optional]\n",
        "\n",
        "Using Income_BDT in C-Data-2, compute Min-Max scaled values. Then compute Robust scaled values. In one line, compare how each treats the 300000 outlier."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "35b830be",
      "metadata": {
        "id": "35b830be"
      },
      "outputs": [],
      "source": [
        "income = df2['Income_BDT'].values\n",
        "\n",
        "print(\"C3. Outlier Stress Test\\n\")\n",
        "print(f\"Income_BDT: {income}\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "# Min-Max Scaling\n",
        "income_minmax = (income - income.min()) / (income.max() - income.min())\n",
        "print(\"\\nMin-Max Scaled:\")\n",
        "print(f\"   {income_minmax}\")\n",
        "print(f\"   Normal values (30k-52k) ‚Üí 0.000 to 0.081 (compressed near 0)\")\n",
        "print(f\"   Outlier (300k) ‚Üí 1.000 (dominates the scale)\")\n",
        "\n",
        "# Robust Scaling\n",
        "median_inc = np.median(income)\n",
        "Q1_inc = np.percentile(income, 25)\n",
        "Q3_inc = np.percentile(income, 75)\n",
        "IQR_inc = Q3_inc - Q1_inc\n",
        "income_robust = (income - median_inc) / IQR_inc\n",
        "\n",
        "print(\"\\nRobust Scaled:\")\n",
        "print(f\"   Median: {median_inc}, IQR: {IQR_inc}\")\n",
        "print(f\"   {income_robust}\")\n",
        "print(f\"   Normal values ‚Üí -1.0 to 0.706 (well distributed)\")\n",
        "print(f\"   Outlier (300k) ‚Üí 17.94 (large but doesn't compress others)\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"üìù ONE LINE COMPARISON:\")\n",
        "print(\"=\"*80)\n",
        "print(\"   Min-Max squashes all normal values to [0, 0.081] because the outlier 300k\")\n",
        "print(\"   sets max, while Robust keeps normal values spread across [-1, 0.7] since\")\n",
        "print(\"   it uses median/IQR unaffected by the extreme value.\")\n",
        "\n",
        "# Visualization\n",
        "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
        "\n",
        "axes[0].bar(range(len(income)), income_minmax, color=['green', 'green', 'green', 'red', 'green'])\n",
        "axes[0].set_title('Min-Max Scaled Income')\n",
        "axes[0].set_xlabel('Sample Index')\n",
        "axes[0].set_ylabel('Scaled Value')\n",
        "axes[0].set_xticks(range(len(income)))\n",
        "axes[0].grid(True, alpha=0.3)\n",
        "\n",
        "axes[1].bar(range(len(income)), income_robust, color=['green', 'green', 'green', 'red', 'green'])\n",
        "axes[1].set_title('Robust Scaled Income')\n",
        "axes[1].set_xlabel('Sample Index')\n",
        "axes[1].set_ylabel('Scaled Value')\n",
        "axes[1].set_xticks(range(len(income)))\n",
        "axes[1].grid(True, alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "723be7af",
      "metadata": {
        "id": "723be7af"
      },
      "source": [
        "## C4. Distance on Feature Space [Optional]\n",
        "\n",
        "From C-Data-1, take feature pair (Hours_Study, GPA)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4ef7d600",
      "metadata": {
        "id": "4ef7d600"
      },
      "outputs": [],
      "source": [
        "# Extract features\n",
        "hours = df1['Hours_Study'].values\n",
        "gpa = df1['GPA'].values\n",
        "\n",
        "# Create feature vectors\n",
        "features = np.column_stack([hours, gpa])\n",
        "ID1 = features[0]  # ID 1: [1.0, 3.10]\n",
        "ID4 = features[3]  # ID 4: [5.0, 3.90]\n",
        "\n",
        "print(\"C4. Distance on Feature Space\\n\")\n",
        "print(f\"Features: (Hours_Study, GPA)\")\n",
        "print(f\"ID 1: {ID1}\")\n",
        "print(f\"ID 4: {ID4}\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "# Task a) Euclidean distance\n",
        "euclidean_orig = np.sqrt((ID4[0] - ID1[0])**2 + (ID4[1] - ID1[1])**2)\n",
        "print(\"\\na) Euclidean Distance (Original)\")\n",
        "print(f\"   d = ‚àö((5.0-1.0)¬≤ + (3.90-3.10)¬≤)\")\n",
        "print(f\"   d = ‚àö(16.0 + 0.64)\")\n",
        "print(f\"   d = ‚àö16.64 = {euclidean_orig:.4f}\")\n",
        "\n",
        "# Task b) Manhattan distance\n",
        "manhattan_orig = abs(ID4[0] - ID1[0]) + abs(ID4[1] - ID1[1])\n",
        "print(\"\\nb) Manhattan Distance (Original)\")\n",
        "print(f\"   d = |5.0-1.0| + |3.90-3.10|\")\n",
        "print(f\"   d = 4.0 + 0.8 = {manhattan_orig:.4f}\")\n",
        "\n",
        "# Task c) Normalize with Min-Max and recompute\n",
        "hours_minmax = (hours - hours.min()) / (hours.max() - hours.min())\n",
        "gpa_minmax = (gpa - gpa.min()) / (gpa.max() - gpa.min())\n",
        "features_scaled = np.column_stack([hours_minmax, gpa_minmax])\n",
        "ID1_scaled = features_scaled[0]\n",
        "ID4_scaled = features_scaled[3]\n",
        "\n",
        "euclidean_scaled = np.sqrt((ID4_scaled[0] - ID1_scaled[0])**2 + (ID4_scaled[1] - ID1_scaled[1])**2)\n",
        "manhattan_scaled = abs(ID4_scaled[0] - ID1_scaled[0]) + abs(ID4_scaled[1] - ID1_scaled[1])\n",
        "\n",
        "print(\"\\nc) After Min-Max Normalization\")\n",
        "print(f\"   ID 1 scaled: {ID1_scaled}\")\n",
        "print(f\"   ID 4 scaled: {ID4_scaled}\")\n",
        "print(f\"\\n   Euclidean Distance (Scaled): {euclidean_scaled:.4f}\")\n",
        "print(f\"   Manhattan Distance (Scaled): {manhattan_scaled:.4f}\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"üìù ONE LINE COMMENT ON SCALE EFFECTS:\")\n",
        "print(\"=\"*80)\n",
        "print(\"   Before scaling, Hours_Study (range 0.2-5.0) dominated distances over GPA\")\n",
        "print(\"   (range 2.3-3.9); Min-Max scaling equalizes feature contributions, reducing\")\n",
        "print(\"   distances and making both features equally important in similarity measures.\")\n",
        "\n",
        "# Create comparison table\n",
        "comparison = pd.DataFrame({\n",
        "    'Metric': ['Euclidean', 'Manhattan'],\n",
        "    'Original': [euclidean_orig, manhattan_orig],\n",
        "    'Scaled': [euclidean_scaled, manhattan_scaled],\n",
        "    'Change': [euclidean_scaled - euclidean_orig, manhattan_scaled - manhattan_orig]\n",
        "})\n",
        "print(\"\\nDistance Comparison:\")\n",
        "print(comparison.to_string(index=False))\n",
        "\n",
        "# Visualization\n",
        "fig, axes = plt.subplots(1, 2, figsize=(14, 6))\n",
        "\n",
        "# Original scale\n",
        "axes[0].scatter(hours, gpa, s=200, c='blue', alpha=0.6, edgecolors='black', linewidths=2)\n",
        "axes[0].scatter([ID1[0], ID4[0]], [ID1[1], ID4[1]], s=300, c=['red', 'green'],\n",
        "               edgecolors='black', linewidths=2, label=['ID 1', 'ID 4'])\n",
        "axes[0].plot([ID1[0], ID4[0]], [ID1[1], ID4[1]], 'k--', linewidth=2,\n",
        "            label=f'Euclidean: {euclidean_orig:.3f}')\n",
        "axes[0].set_xlabel('Hours_Study')\n",
        "axes[0].set_ylabel('GPA')\n",
        "axes[0].set_title('Original Scale')\n",
        "axes[0].legend()\n",
        "axes[0].grid(True, alpha=0.3)\n",
        "\n",
        "# Scaled\n",
        "axes[1].scatter(hours_minmax, gpa_minmax, s=200, c='blue', alpha=0.6, edgecolors='black', linewidths=2)\n",
        "axes[1].scatter([ID1_scaled[0], ID4_scaled[0]], [ID1_scaled[1], ID4_scaled[1]],\n",
        "               s=300, c=['red', 'green'], edgecolors='black', linewidths=2, label=['ID 1', 'ID 4'])\n",
        "axes[1].plot([ID1_scaled[0], ID4_scaled[0]], [ID1_scaled[1], ID4_scaled[1]], 'k--',\n",
        "            linewidth=2, label=f'Euclidean: {euclidean_scaled:.3f}')\n",
        "axes[1].set_xlabel('Hours_Study (scaled)')\n",
        "axes[1].set_ylabel('GPA (scaled)')\n",
        "axes[1].set_title('Min-Max Scaled [0, 1]')\n",
        "axes[1].legend()\n",
        "axes[1].grid(True, alpha=0.3)\n",
        "axes[1].set_xlim(-0.1, 1.1)\n",
        "axes[1].set_ylim(-0.1, 1.1)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "10fc36bb",
      "metadata": {
        "id": "10fc36bb"
      },
      "source": [
        "# Part D. Mini Project [Optional]\n",
        "\n",
        "**Goal:** Make one notebook that shows encoding + scaling + distance change. No train‚Äìtest split, no models.\n",
        "\n",
        "## Step 1: Create a Small DataFrame"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b8328ead",
      "metadata": {
        "id": "b8328ead"
      },
      "outputs": [],
      "source": [
        "# Create a comprehensive dataset for the mini project\n",
        "project_data = {\n",
        "    'Income': [35000, 48000, 52000, 250000, 42000, 38000],\n",
        "    'Hours_Study': [2.5, 4.0, 3.5, 5.0, 1.5, 3.0],\n",
        "    'GPA': [3.2, 3.7, 3.5, 3.9, 2.8, 3.3],\n",
        "    'Transactions_7d': [5, 8, 12, 85, 3, 6],\n",
        "    'City': ['Dhaka', 'Chattogram', 'Dhaka', 'Rajshahi', 'Dhaka', 'Chattogram'],\n",
        "    'Internet': ['Yes', 'Yes', 'No', 'Yes', 'Yes', 'No'],\n",
        "    'Education_Level': ['Bachelor', 'Master', 'Bachelor', 'PhD', 'High School', 'Master'],\n",
        "    'Satisfaction': ['Medium', 'High', 'Medium', 'High', 'Low', 'Medium']\n",
        "}\n",
        "\n",
        "df_project = pd.DataFrame(project_data)\n",
        "print(\"Mini Project Dataset:\")\n",
        "print(df_project)\n",
        "print(f\"\\nShape: {df_project.shape}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "01c93746",
      "metadata": {
        "id": "01c93746"
      },
      "source": [
        "## Step 2: Decide Preprocessing Plan"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4a03d0a4",
      "metadata": {
        "id": "4a03d0a4"
      },
      "outputs": [],
      "source": [
        "print(\"Preprocessing Plan\\n\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "print(\"\\nüìã One-Hot Encoding (Nominal Variables):\")\n",
        "print(\"   ‚Ä¢ City: {Dhaka, Chattogram, Rajshahi} - no natural order\")\n",
        "print(\"   ‚Ä¢ Internet: {Yes, No} - binary nominal\")\n",
        "\n",
        "print(\"\\nüìã Ordinal Encoding (Ordinal Variables):\")\n",
        "print(\"   ‚Ä¢ Education_Level: High School < Bachelor < Master < PhD\")\n",
        "print(\"     Mapping: {'High School': 0, 'Bachelor': 1, 'Master': 2, 'PhD': 3}\")\n",
        "print(\"   ‚Ä¢ Satisfaction: Low < Medium < High\")\n",
        "print(\"     Mapping: {'Low': 0, 'Medium': 1, 'High': 2}\")\n",
        "\n",
        "print(\"\\nüìã Standardization (Normal Distribution Expected):\")\n",
        "print(\"   ‚Ä¢ Hours_Study: continuous variable, no severe outliers\")\n",
        "print(\"   ‚Ä¢ GPA: bounded but continuous\")\n",
        "\n",
        "print(\"\\nüìã Robust Scaling (Outliers Present):\")\n",
        "print(\"   ‚Ä¢ Income: outlier at 250000 (6√ó larger than others)\")\n",
        "print(\"   ‚Ä¢ Transactions_7d: outlier at 85 (10√ó larger than others)\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4a8aadbc",
      "metadata": {
        "id": "4a8aadbc"
      },
      "source": [
        "## Step 3: Apply ColumnTransformer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5ade5e54",
      "metadata": {
        "id": "5ade5e54"
      },
      "outputs": [],
      "source": [
        "from sklearn.preprocessing import StandardScaler, RobustScaler, OneHotEncoder, OrdinalEncoder\n",
        "from sklearn.compose import ColumnTransformer\n",
        "\n",
        "# Define column groups\n",
        "onehot_cols = ['City', 'Internet']\n",
        "ordinal_cols = ['Education_Level', 'Satisfaction']\n",
        "standard_cols = ['Hours_Study', 'GPA']\n",
        "robust_cols = ['Income', 'Transactions_7d']\n",
        "\n",
        "# Define ordinal mappings\n",
        "education_mapping = [['High School', 'Bachelor', 'Master', 'PhD']]\n",
        "satisfaction_mapping = [['Low', 'Medium', 'High']]\n",
        "\n",
        "# Create ColumnTransformer\n",
        "preprocessor = ColumnTransformer(\n",
        "    transformers=[\n",
        "        ('onehot', OneHotEncoder(sparse_output=False, drop='first'), onehot_cols),\n",
        "        ('ordinal_edu', OrdinalEncoder(categories=education_mapping), ['Education_Level']),\n",
        "        ('ordinal_sat', OrdinalEncoder(categories=satisfaction_mapping), ['Satisfaction']),\n",
        "        ('standard', StandardScaler(), standard_cols),\n",
        "        ('robust', RobustScaler(), robust_cols)\n",
        "    ]\n",
        ")\n",
        "\n",
        "# Fit and transform\n",
        "transformed = preprocessor.fit_transform(df_project)\n",
        "\n",
        "print(\"Transformed Data Shape:\", transformed.shape)\n",
        "print(\"\\nFirst 3 rows of transformed data:\")\n",
        "print(transformed[:3])\n",
        "\n",
        "# Get feature names\n",
        "feature_names = []\n",
        "# OneHot features\n",
        "feature_names.extend(preprocessor.named_transformers_['onehot'].get_feature_names_out(onehot_cols))\n",
        "# Ordinal features\n",
        "feature_names.extend(['Education_Level', 'Satisfaction'])\n",
        "# Standard features\n",
        "feature_names.extend(standard_cols)\n",
        "# Robust features\n",
        "feature_names.extend(robust_cols)\n",
        "\n",
        "print(f\"\\nFeature names ({len(feature_names)} total):\")\n",
        "print(feature_names)\n",
        "\n",
        "# Create DataFrame for better visualization\n",
        "df_transformed = pd.DataFrame(transformed, columns=feature_names)\n",
        "print(\"\\nTransformed DataFrame:\")\n",
        "print(df_transformed)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7f787d98",
      "metadata": {
        "id": "7f787d98"
      },
      "source": [
        "## Step 4: Distance Before vs After Scaling\n",
        "\n",
        "Pick two numeric columns: (Income, Transactions_7d). Take 3 rows: P1, P2, P3."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2d43e69a",
      "metadata": {
        "id": "2d43e69a"
      },
      "outputs": [],
      "source": [
        "# Select rows 0, 1, 3 (to include the outlier)\n",
        "selected_rows = [0, 1, 3]\n",
        "P1_orig = df_project.loc[0, ['Income', 'Transactions_7d']].values\n",
        "P2_orig = df_project.loc[1, ['Income', 'Transactions_7d']].values\n",
        "P3_orig = df_project.loc[3, ['Income', 'Transactions_7d']].values  # outlier row\n",
        "\n",
        "print(\"Step 4: Distance Before vs After Scaling\\n\")\n",
        "print(\"=\"*80)\n",
        "print(\"\\nSelected Points (Income, Transactions_7d):\")\n",
        "print(f\"P1 (row 0): {P1_orig}\")\n",
        "print(f\"P2 (row 1): {P2_orig}\")\n",
        "print(f\"P3 (row 3): {P3_orig} ‚Üê outlier\")\n",
        "\n",
        "# Compute original distances\n",
        "def euclidean(a, b):\n",
        "    return np.sqrt(np.sum((a - b)**2))\n",
        "\n",
        "def manhattan(a, b):\n",
        "    return np.sum(np.abs(a - b))\n",
        "\n",
        "print(\"\\n--- BEFORE SCALING ---\")\n",
        "euc_12_orig = euclidean(P1_orig, P2_orig)\n",
        "euc_13_orig = euclidean(P1_orig, P3_orig)\n",
        "euc_23_orig = euclidean(P2_orig, P3_orig)\n",
        "\n",
        "man_12_orig = manhattan(P1_orig, P2_orig)\n",
        "man_13_orig = manhattan(P1_orig, P3_orig)\n",
        "man_23_orig = manhattan(P2_orig, P3_orig)\n",
        "\n",
        "print(f\"\\nEuclidean Distances:\")\n",
        "print(f\"  P1-P2: {euc_12_orig:.2f}\")\n",
        "print(f\"  P1-P3: {euc_13_orig:.2f}\")\n",
        "print(f\"  P2-P3: {euc_23_orig:.2f}\")\n",
        "\n",
        "print(f\"\\nManhattan Distances:\")\n",
        "print(f\"  P1-P2: {man_12_orig:.2f}\")\n",
        "print(f\"  P1-P3: {man_13_orig:.2f}\")\n",
        "print(f\"  P2-P3: {man_23_orig:.2f}\")\n",
        "\n",
        "# Apply two different scalers\n",
        "# Scaler 1: StandardScaler\n",
        "scaler_std = StandardScaler()\n",
        "data_std = scaler_std.fit_transform(df_project[['Income', 'Transactions_7d']])\n",
        "P1_std = data_std[0]\n",
        "P2_std = data_std[1]\n",
        "P3_std = data_std[3]\n",
        "\n",
        "print(\"\\n--- AFTER STANDARDIZATION ---\")\n",
        "euc_12_std = euclidean(P1_std, P2_std)\n",
        "euc_13_std = euclidean(P1_std, P3_std)\n",
        "euc_23_std = euclidean(P2_std, P3_std)\n",
        "\n",
        "man_12_std = manhattan(P1_std, P2_std)\n",
        "man_13_std = manhattan(P1_std, P3_std)\n",
        "man_23_std = manhattan(P2_std, P3_std)\n",
        "\n",
        "print(f\"\\nEuclidean Distances:\")\n",
        "print(f\"  P1-P2: {euc_12_std:.4f}\")\n",
        "print(f\"  P1-P3: {euc_13_std:.4f}\")\n",
        "print(f\"  P2-P3: {euc_23_std:.4f}\")\n",
        "\n",
        "print(f\"\\nManhattan Distances:\")\n",
        "print(f\"  P1-P2: {man_12_std:.4f}\")\n",
        "print(f\"  P1-P3: {man_13_std:.4f}\")\n",
        "print(f\"  P2-P3: {man_23_std:.4f}\")\n",
        "\n",
        "# Scaler 2: RobustScaler\n",
        "scaler_rob = RobustScaler()\n",
        "data_rob = scaler_rob.fit_transform(df_project[['Income', 'Transactions_7d']])\n",
        "P1_rob = data_rob[0]\n",
        "P2_rob = data_rob[1]\n",
        "P3_rob = data_rob[3]\n",
        "\n",
        "print(\"\\n--- AFTER ROBUST SCALING ---\")\n",
        "euc_12_rob = euclidean(P1_rob, P2_rob)\n",
        "euc_13_rob = euclidean(P1_rob, P3_rob)\n",
        "euc_23_rob = euclidean(P2_rob, P3_rob)\n",
        "\n",
        "man_12_rob = manhattan(P1_rob, P2_rob)\n",
        "man_13_rob = manhattan(P1_rob, P3_rob)\n",
        "man_23_rob = manhattan(P2_rob, P3_rob)\n",
        "\n",
        "print(f\"\\nEuclidean Distances:\")\n",
        "print(f\"  P1-P2: {euc_12_rob:.4f}\")\n",
        "print(f\"  P1-P3: {euc_13_rob:.4f}\")\n",
        "print(f\"  P2-P3: {euc_23_rob:.4f}\")\n",
        "\n",
        "print(f\"\\nManhattan Distances:\")\n",
        "print(f\"  P1-P2: {man_12_rob:.4f}\")\n",
        "print(f\"  P1-P3: {man_13_rob:.4f}\")\n",
        "print(f\"  P2-P3: {man_23_rob:.4f}\")\n",
        "\n",
        "# Summary table\n",
        "summary_table = pd.DataFrame({\n",
        "    'Pair': ['P1-P2', 'P1-P3', 'P2-P3', 'P1-P2', 'P1-P3', 'P2-P3'],\n",
        "    'Metric': ['Euclidean', 'Euclidean', 'Euclidean', 'Manhattan', 'Manhattan', 'Manhattan'],\n",
        "    'Original': [euc_12_orig, euc_13_orig, euc_23_orig, man_12_orig, man_13_orig, man_23_orig],\n",
        "    'Standard': [euc_12_std, euc_13_std, euc_23_std, man_12_std, man_13_std, man_23_std],\n",
        "    'Robust': [euc_12_rob, euc_13_rob, euc_23_rob, man_12_rob, man_13_rob, man_23_rob]\n",
        "})\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"SUMMARY TABLE\")\n",
        "print(\"=\"*80)\n",
        "print(summary_table.to_string(index=False))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e9fb4f76",
      "metadata": {
        "id": "e9fb4f76"
      },
      "source": [
        "## Step 5: Short Reflection"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0ae39a18",
      "metadata": {
        "id": "0ae39a18"
      },
      "outputs": [],
      "source": [
        "print(\"=\"*80)\n",
        "print(\"REFLECTION: Mini Project Insights\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "print(\"\\n1Ô∏è‚É£ Which scaler handled outliers better?\")\n",
        "print(\"   ‚Üí Robust Scaler performed better for features with outliers (Income, Transactions_7d).\")\n",
        "print(\"   ‚Üí It uses median and IQR instead of mean and std, making it resistant to extreme values.\")\n",
        "print(\"   ‚Üí Standard Scaler was influenced by outliers, inflating the standard deviation and\")\n",
        "print(\"     compressing the normal values closer together than they should be.\")\n",
        "\n",
        "print(\"\\n2Ô∏è‚É£ Did scaling change which points are 'closer' to each other?\")\n",
        "print(\"   ‚Üí Yes! Before scaling, Income (range ~200k) dominated over Transactions (range ~80),\")\n",
        "print(\"     making P3 (outlier in both) seem very far from P1 and P2.\")\n",
        "print(\"   ‚Üí After scaling, the relative distances changed; features contribute more equally.\")\n",
        "print(\"   ‚Üí Robust scaling preserved relative ordering better than StandardScaler for normal points.\")\n",
        "\n",
        "print(\"\\n3Ô∏è‚É£ Why does this matter for algorithms that use distance?\")\n",
        "print(\"   ‚Üí Distance-based algorithms (KNN, K-Means, SVM with RBF) assume all features are\")\n",
        "print(\"     on comparable scales; unscaled features let high-magnitude features dominate.\")\n",
        "print(\"   ‚Üí Scaling ensures each feature contributes fairly to similarity/distance calculations.\")\n",
        "print(\"   ‚Üí Choosing the right scaler (Robust for outliers, Standard for normal distributions,\")\n",
        "print(\"     Min-Max for bounded ranges) prevents distortion and improves model performance.\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "\n",
        "# Visualization\n",
        "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
        "\n",
        "# Original\n",
        "axes[0].scatter(df_project['Income'], df_project['Transactions_7d'], s=200, alpha=0.6)\n",
        "axes[0].scatter([P1_orig[0], P2_orig[0], P3_orig[0]],\n",
        "               [P1_orig[1], P2_orig[1], P3_orig[1]],\n",
        "               s=300, c=['red', 'blue', 'green'], edgecolors='black', linewidths=2,\n",
        "               label=['P1', 'P2', 'P3 (outlier)'])\n",
        "axes[0].set_xlabel('Income (BDT)')\n",
        "axes[0].set_ylabel('Transactions_7d')\n",
        "axes[0].set_title('Original Scale\\n(Income dominates)')\n",
        "axes[0].legend()\n",
        "axes[0].grid(True, alpha=0.3)\n",
        "\n",
        "# Standard Scaler\n",
        "axes[1].scatter(data_std[:, 0], data_std[:, 1], s=200, alpha=0.6)\n",
        "axes[1].scatter([P1_std[0], P2_std[0], P3_std[0]],\n",
        "               [P1_std[1], P2_std[1], P3_std[1]],\n",
        "               s=300, c=['red', 'blue', 'green'], edgecolors='black', linewidths=2,\n",
        "               label=['P1', 'P2', 'P3 (outlier)'])\n",
        "axes[1].set_xlabel('Income (standardized)')\n",
        "axes[1].set_ylabel('Transactions_7d (standardized)')\n",
        "axes[1].set_title('Standard Scaler\\n(Outlier inflates std)')\n",
        "axes[1].legend()\n",
        "axes[1].grid(True, alpha=0.3)\n",
        "\n",
        "# Robust Scaler\n",
        "axes[2].scatter(data_rob[:, 0], data_rob[:, 1], s=200, alpha=0.6)\n",
        "axes[2].scatter([P1_rob[0], P2_rob[0], P3_rob[0]],\n",
        "               [P1_rob[1], P2_rob[1], P3_rob[1]],\n",
        "               s=300, c=['red', 'blue', 'green'], edgecolors='black', linewidths=2,\n",
        "               label=['P1', 'P2', 'P3 (outlier)'])\n",
        "axes[2].set_xlabel('Income (robust scaled)')\n",
        "axes[2].set_ylabel('Transactions_7d (robust scaled)')\n",
        "axes[2].set_title('Robust Scaler\\n(Best for outliers)')\n",
        "axes[2].legend()\n",
        "axes[2].grid(True, alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}